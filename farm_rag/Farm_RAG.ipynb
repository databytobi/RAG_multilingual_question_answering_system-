{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notebook Documentation\n",
        "This notebook demonstrates building a question-answering system that can respond in multiple languages (specifically Swahili and English in this example) using a combination of Retrieval Augmented Generation (RAG) and an agent with web search capabilities."
      ],
      "metadata": {
        "id": "Hb_Jy2krB-nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell installs all the required Python libraries for the project, including:\n",
        "\n",
        "langchain: The core library for building LLM applications.\n",
        "faiss-cpu: A library for efficient similarity search and clustering of dense vectors, used here for the vector store.\n",
        "transformers: Used for loading pre-trained models, specifically for the translation models and embeddings.\n",
        "torch: The deep learning framework, required by transformers.\n",
        "serpapi: (Originally intended for SerpAPI, later replaced by DuckDuckGo) A Python client for the SerpAPI search API.\n",
        "google-genai: The Google Generative AI SDK.\n",
        "python-dotenv: Used for loading environment variables (though later switched to Colab secrets).\n",
        "langchain-community: Contains community-contributed LangChain components, including tools like SerpAPIWrapper and DuckDuckGoSearchRun.\n",
        "langchain_google_genai: LangChain integration for Google's Generative AI models.\n",
        "google-search-results: A dependency for SerpAPI."
      ],
      "metadata": {
        "id": "ciB895jHCSDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C4tJOKVhSQO",
        "outputId": "df3a8e2e-0e03-44d3-f6f8-841abf4432aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: serpapi in /usr/local/lib/python3.11/dist-packages (0.1.5)\n",
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.29.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.1.9)\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.11/dist-packages (2.4.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.10.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.6.18)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install langchain faiss-cpu transformers torch serpapi google-genai python-dotenv langchain-community langchain_google_genai google-search-results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads the necessary API keys from Google Colab's Secrets Manager. It retrieves the SERPAPI_API_KEY (though no longer used directly after switching to DuckDuckGo) and GOOGLE_API_KEY. It then prints a masked version of the keys to confirm they were loaded."
      ],
      "metadata": {
        "id": "lrc-IUJMCanM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# load_dotenv() # No longer needed when using userdata\n",
        "\n",
        "#SERPAPI_API_KEY = userdata.get(\"SERPAPI_API_KEY\")\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "\n",
        "#print(f\"SerpAPI Key: {SERPAPI_API_KEY[:2]}...\" if SERPAPI_API_KEY else \"SerpAPI Key not found\")\n",
        "print(f\"Gemini API Key: {GOOGLE_API_KEY[:2]}...\" if GOOGLE_API_KEY else \"Google API Key not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mse_C3LthkvN",
        "outputId": "b5a1611f-5f29-4f1e-e543-83ae12490a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SerpAPI Key: c5...\n",
            "Gemini API Key: AI...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports various classes and functions needed for building the RAG system and the agent:\n",
        "\n",
        "TextLoader, RecursiveCharacterTextSplitter: For loading and splitting text documents (though dummy data is used later).\n",
        "\n",
        "HuggingFaceEmbeddings: For creating vector embeddings of text using a Hugging Face model.\n",
        "\n",
        "FAISS: For creating a vector store from the embeddings.\n",
        "\n",
        "RetrievalQA: A LangChain chain for performing question answering over retrieved documents.\n",
        "PromptTemplate: For defining the structure of prompts sent to the language model.\n",
        "\n",
        "SerpAPIWrapper: (Originally imported, later replaced in the tools list) A LangChain tool for using SerpAPI for web search.\n",
        "\n",
        "initialize_agent, Tool: For creating and configuring the LangChain agent.\n",
        "\n",
        "google.generativeai: The Google Generative AI SDK, used directly for some operations.\n",
        "\n",
        "MarianMTModel, MarianTokenizer: From transformers, used for loading and using Marian translation models.\n",
        "\n",
        "torch: For managing tensors and device placement for the translation models.This cell imports various classes and functions needed for building the RAG system and the agent:\n",
        "\n"
      ],
      "metadata": {
        "id": "yTHt7nSICyce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for LangChain and other systems\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.utilities import SerpAPIWrapper # Corrected import\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "import google.generativeai as genai\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "#from serpapi import GoogleSearch"
      ],
      "metadata": {
        "id": "_7jCEysXiSwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell configures the Google Generative AI client using the GOOGLE_API_KEY loaded from secrets. This is necessary before making any calls to the Gemini API."
      ],
      "metadata": {
        "id": "U-T3NPPLDR3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Gemini API client\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "else:\n",
        "    print(\"Warning: Google API key is missing.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PKSpJSc-i-4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads the MarianMT model and tokenizer for translation from Chinese to English (Helsinki-NLP/opus-mt-zh-en). Note: This model name is incorrect for Swahili to English and should be Helsinki-NLP/opus-mt-sw-en (if available) or a different appropriate model. The code attempts to move the model to a GPU if available."
      ],
      "metadata": {
        "id": "QLfXEP5eDcqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup translation pipeline placeholders\n",
        "\n",
        "# Load translation model and tokenizer for Swahili to English (example)\n",
        "mt_model_name_sw_en = \"Helsinki-NLP/opus-mt-zh-en\"\n",
        "mt_tokenizer_sw_en = MarianTokenizer.from_pretrained(mt_model_name_sw_en)\n",
        "mt_model_sw_en = MarianMTModel.from_pretrained(mt_model_name_sw_en).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Fa0OwsllcM",
        "outputId": "cee28990-1ddc-4094-dd34-a012f291dd96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines a Python function translate_to_english that takes text as input, tokenizes it using the loaded Swahili-to-English tokenizer, generates the translated text using the corresponding model, and decodes the output."
      ],
      "metadata": {
        "id": "z1De8WAHDqCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate to English\n",
        "def translate_to_english(text):\n",
        "    inputs = mt_tokenizer_sw_en(text, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(mt_model_sw_en.device) for k, v in inputs.items()}\n",
        "    translated = mt_model_sw_en.generate(**inputs)\n",
        "    return mt_tokenizer_sw_en.decode(translated[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "1qp6MmOlmJUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell initializes placeholder variables for the English to Swahili translation model and tokenizer. They are set to None and will be loaded when the translate_from_english function is called for the first time."
      ],
      "metadata": {
        "id": "IgjCV5BRD3W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for English to Swahili (or other language), will init later\n",
        "mt_model_en_sw = None\n",
        "mt_tokenizer_en_sw = None\n"
      ],
      "metadata": {
        "id": "Yx0QBCMMnkmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the translate_from_english function. It checks if the English to target language (defaulting to Swahili) model and tokenizer are already loaded. If not, it loads the appropriate MarianMT model (Helsinki-NLP/opus-mt-en-{target_language}) and tokenizer from Hugging Face and moves the model to the appropriate device. It then performs the translation similar to the translate_to_english function."
      ],
      "metadata": {
        "id": "bjU4YzasECVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate from English\n",
        "def translate_from_english(text, target_language=\"sw\"):\n",
        "    global mt_model_en_sw, mt_tokenizer_en_sw\n",
        "    if mt_model_en_sw is None or mt_tokenizer_en_sw is None:\n",
        "        model_name = f\"Helsinki-NLP/opus-mt-en-{target_language}\" # Corrected model name\n",
        "        mt_tokenizer_en_sw = MarianTokenizer.from_pretrained(model_name)\n",
        "        mt_model_en_sw = MarianMTModel.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = mt_tokenizer_en_sw(text, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(mt_model_en_sw.device) for k, v in inputs.items()}\n",
        "    translated = mt_model_en_sw.generate(**inputs)\n",
        "    return mt_tokenizer_en_sw.decode(translated[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "W4LLzPn6nrcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make this notebook smaller and runnable in a typical environment,\n",
        "we will simulate document loading with dummy data in the next cell.\n"
      ],
      "metadata": {
        "id": "xY-CdNCKoHw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell contains a Python list sample_docs with strings representing short pieces of text about farming practices. This list serves as the \"documents\" for the RAG system."
      ],
      "metadata": {
        "id": "GMjxhPnNEZ-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate loading documents\n",
        "sample_docs = [\n",
        "    \"Crop rotation is essential to maintain soil health and increase yield.\",\n",
        "    \"Water management helps avoid drought stress in plants.\",\n",
        "    \"Use organic fertilizers to enrich soil nutrients.\",\n",
        "    \"Timely planting improves harvest success.\",\n",
        "    \"Improving soil fertility through cover cropping and adding compost can significantly increase crop yields.\",\n",
        "    \"Proper pest and disease management is crucial for maximizing harvest size.\",\n",
        "    \"Selecting high-yielding and disease-resistant crop varieties is key to increasing productivity.\",\n",
        "    \"Optimizing planting density and spacing can improve resource utilization and lead to higher yields.\",\n",
        "    \"Effective weed control reduces competition for nutrients and water, boosting crop production.\"\n",
        "]"
      ],
      "metadata": {
        "id": "UnUJs3_Zn60S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell converts the strings in sample_docs into Document objects, which is the format LangChain works with. It then uses RecursiveCharacterTextSplitter to split these documents into smaller chunks of a specified size with overlap. This is done to ensure that the chunks are small enough to be processed by the embedding model and to maintain context between chunks."
      ],
      "metadata": {
        "id": "Fpc4fiUQEmtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Document objects\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [Document(page_content=text) for text in sample_docs]\n",
        "\n",
        "# Split documents into chunks for embedding\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "len(docs)  # number of chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttVOaCtboU7M",
        "outputId": "93a38850-ff0b-45d3-ce60-f8bb33eab4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell initializes a HuggingFaceEmbeddings model using \"sentence-transformers/all-MiniLM-L6-v2\" to create vector representations (embeddings) of the text chunks. It then uses FAISS.from_documents to create a FAISS vector store from the document chunks and their embeddings. This vector store allows for efficient similarity search."
      ],
      "metadata": {
        "id": "4-iVWp18Ewn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings and vectorstore using Faiss\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(docs, embedding_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIkKaW9-owEb",
        "outputId": "a91e2af0-2560-4622-91a2-752763009aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-684200172.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell creates a retriever from the FAISS vector store. The retriever is configured to use \"similarity\" search and retrieve the top 3 most similar documents (search_kwargs={\"k\":3}) when queried."
      ],
      "metadata": {
        "id": "TzyWm_YxFV4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
        "\n"
      ],
      "metadata": {
        "id": "WMZk_o_DozkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the Language Model (LLM) to be used. It uses ChatGoogleGenerativeAI from langchain_google_genai to wrap the \"gemini-2.0-flash\" model. A temperature of 0.0 is set for deterministic output. The GOOGLE_API_KEY is passed for authentication."
      ],
      "metadata": {
        "id": "_wms6AOpFg-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Gemini LLM wrapper\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Use ChatGoogleGenerativeAI as the LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.0, google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "6i_KQ0kHo8qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the prompt template for the RAG chain. It instructs the LLM to act as a helpful farming assistant and use the provided context to answer the question in simple language. It then creates a RetrievalQA chain, connecting the LLM, the retriever, and the prompt. chain_type=\"stuff\" means all retrieved documents will be stuffed into the prompt. return_source_documents=True is set to get the source documents along with the answer."
      ],
      "metadata": {
        "id": "WcOiTst5FsTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Prompt Template and RetrievalQA Chain\n",
        "\n",
        "template = '''\n",
        "You are a helpful farming assistant. Use the context below to answer the question.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in simple clear language.\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm, retriever=retriever, chain_type=\"stuff\", return_source_documents=True, chain_type_kwargs={\"prompt\": prompt}\n",
        ")"
      ],
      "metadata": {
        "id": "rMAFegtdpW7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEzyXDnA2coM",
        "outputId": "82918b58-fc3c-45b9-a1c0-c6006d40f7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Downloading duckduckgo_search-8.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo-search\n",
            "Successfully installed duckduckgo-search-8.1.1 primp-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell initializes the DuckDuckGoSearchRun tool from langchain_community.tools. This tool allows the agent to perform web searches using DuckDuckGo. A list of tools is created, containing this \"Web Search\" tool."
      ],
      "metadata": {
        "id": "U661GDg6F3vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup DuckDuckGo Search Tool\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "tools = [\n",
        "    Tool(name=\"Web Search\", func=search.run, description=\"Use this for searching up-to-date info on the web.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "tExEkLMKphfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell initializes a LangChain agent. It uses the initialize_agent function with the defined tools and the LLM. agent=\"zero-shot-react-description\" specifies the agent type, which uses the ReAct framework for reasoning and acting. verbose=True is set to show the agent's thought process during execution."
      ],
      "metadata": {
        "id": "YnusbRudGDq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize agent combining Gemini LLM and ducks\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "hN4UYyY6qPB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the main answer_question function. It takes a user question and target language (defaulting to English, though intended for Swahili) as input.\n",
        "\n",
        "It translates the question to English if the input language is not English.\n",
        "It runs the qa_chain (RAG) with the English query.\n",
        "It checks if source_documents were returned by the RAG.\n",
        "If no source documents were found, it prints a message and invokes the agent_executor with the English query to perform a web search.\n",
        "If source documents were found, it uses the RAG answer.\n",
        "Finally, it translates the final English answer back to the user's original language if it was not English."
      ],
      "metadata": {
        "id": "yDXvSb7IGNXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(user_question, user_language=\"sw\"):\n",
        "    if user_language != \"sw\":\n",
        "        query_en = translate_to_english(user_question)\n",
        "    else:\n",
        "        query_en = user_question\n",
        "\n",
        "    # Run RAG first\n",
        "    rag_output = qa_chain.invoke({\"query\": query_en}) # Use invoke and pass query as a dictionary\n",
        "    rag_answer = rag_output['result'] # Access the answer using the 'result' key\n",
        "    source_documents = rag_output['source_documents'] # Get source documents\n",
        "\n",
        "    # Trigger web search if no source documents were found by RAG\n",
        "    if not source_documents:\n",
        "        print(\"RAG did not find relevant documents, falling back to web search...\")\n",
        "        web_answer = agent_executor.invoke({\"input\": query_en})['output'] # Use agent_executor.invoke\n",
        "        final_answer = web_answer\n",
        "    else:\n",
        "        final_answer = rag_answer\n",
        "\n",
        "    if user_language != \"sw\":\n",
        "        translated_answer = translate_from_english(final_answer, target_language=user_language)\n",
        "        return translated_answer\n",
        "    else:\n",
        "        return final_answer"
      ],
      "metadata": {
        "id": "c6IjrjVwsvkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell provides an example of calling the answer_question function with a question in Swahili (\"Jinsi ya kuongeza mazao?\") and prints the translated answer."
      ],
      "metadata": {
        "id": "0UqAPjFgGaNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "\n",
        "question_sw = \"Jinsi ya kuongeza mazao?\"  # Swahili for \"How to increase crop yields?\"\n",
        "ans = answer_question(question_sw, user_language=\"sw\")\n",
        "print(\"Answer:\", ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rukHixXLs3AZ",
        "outputId": "9eb26e38-1269-420c-fe78-43f61acb1b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: I'm working perfectly fine! I'm here to help you with your farming by reminding you about important things like using organic fertilizers, rotating crops, and planting on time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell re-initializes the LangChain agent with an updated prompt template. The prompt is modified to guide the agent more explicitly on when and how to use the \"Web Search\" tool, particularly when the RAG context is insufficient. It also includes instructions to avoid putting square brackets around the tool name in the \"Action\" step and adds handle_parsing_errors=True to the AgentExecutor for better error handling."
      ],
      "metadata": {
        "id": "BfEYCJcAGipb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5af40a50"
      },
      "source": [
        "# Initialize the Agent with an improved prompt\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain_core.prompts import PromptTemplate # Added import\n",
        "\n",
        "agent_prompt = PromptTemplate.from_template(\"\"\"\n",
        "Answer the following questions as best you can, focusing on providing helpful farming information. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "The RAG context from the provided documents is: {agent_scratchpad}\n",
        "\n",
        "If the provided RAG context is empty or does not contain relevant information to answer the question, use the Web Search tool to find the necessary information. **When using Web Search due to insufficient RAG context, base your answer solely on the information found through the web search.** Formulate your search query to be specific and include keywords related to the user's farming question.\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think step by step about what you are going to do. Consider if the RAG context is sufficient or if a web search is needed.\n",
        "Action: the action to take, should be one of [{tool_names}] (do not include square brackets around the tool name in the Action)\n",
        "Action Input: the input to the action (e.g., the search query for Web Search)\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:\"\"\")\n",
        "\n",
        "agent = create_react_agent(llm, tools, agent_prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a96e9205",
        "outputId": "00b2981f-cacb-42b4-e752-56bb3cfe18c4"
      },
      "source": [
        "# Example question for the agent\n",
        "question = \"What are the best ways to improve soil health?\"\n",
        "answer = answer_question(question)\n",
        "print(answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To make your soil healthy, here's what you can do:\n",
            "\n",
            "*   **Use organic fertilizers:** These feed the soil naturally.\n",
            "*   **Rotate your crops:** Plant different crops in the same area each season.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cb57033",
        "outputId": "5a0aa00e-87cf-4ebc-c213-45474c5125b7"
      },
      "source": [
        "# Example in Swahili\n",
        "question_sw = \"Jinsi ya kuongeza mazao?\"  # Swahili for \"How to increase crop yields?\"\n",
        "ans = answer_question(question_sw, user_language=\"sw\")\n",
        "print(\"Answer:\", ans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Ili kuongeza mazao:\n",
            "\n",
            "*   Panda kwa wakati unaofaa.\n",
            "*   Hakikisha mimea inapata maji ya kutosha.\n",
            "*   Tumia mzunguko wa mazao.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example in Swahili\n",
        "question_sw = \"Unaweza kuongezaje mavuno ya mazao kwenye shamba lako??\"  # Swahili for \"How to increase crop yields?\"\n",
        "ans = answer_question(question_sw, user_language=\"sw\")\n",
        "print(\"Answer:\", ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wVe0nSzv9D-",
        "outputId": "793e9980-23dc-402f-83ef-4da49919459e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Ninafanya kazi vizuri kabisa! Nipo hapa kukusaidia na ukulima wenu kwa kuwakumbusha mambo muhimu kama vile kutumia mbolea za kikaboni, kuzungusha mazao, na kupanda kwa wakati.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example in Swahili\n",
        "question_sw = \"Unaweza kudhibiti vipi wadudu waharibifu kwenye shamba langu?\"  # Swahili for \"How to increase crop yields?\"\n",
        "ans = answer_question(question_sw, user_language=\"sw\")\n",
        "print(\"Answer:\", ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVgrMsc8wqkS",
        "outputId": "0b5c3ee0-65fd-496e-968e-590d39d90bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Samahani, sijapewa taarifa kuhusu udhibiti wa wadudu waharibifu. Nimepewa taarifa kuhusu mbolea ya asili, mzunguko wa mazao, na upandaji kwa wakati.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be954b1d",
        "outputId": "482a6312-5a3b-41c5-ff27-f6414b6bbd70"
      },
      "source": [
        "# Test the agent directly with a question that requires web search\n",
        "print(\"Testing agent with web search query...\")\n",
        "agent_response = agent_executor.invoke({\"input\": \"What are the latest advancements in drought-resistant crops?\"})\n",
        "print(\"Agent Output:\", agent_response['output'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing agent with web search query...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe question asks about the latest advancements in drought-resistant crops. This is a rapidly evolving field, so I will use a web search to find the most up-to-date information.\n",
            "Action: [Web Search]\n",
            "Action Input: \"latest advancements drought resistant crops\"\u001b[0m[Web Search] is not a valid tool, try one of [Web Search].\u001b[32;1m\u001b[1;3mThe question asks about the latest advancements in drought-resistant crops. This is a rapidly evolving field, so I will use a web search to find the most up-to-date information.\n",
            "Action: [Web Search]\n",
            "Action Input: \"latest advancements in drought-resistant crops\"\u001b[0m[Web Search] is not a valid tool, try one of [Web Search].\u001b[32;1m\u001b[1;3mThe question asks about the latest advancements in drought-resistant crops. This is a rapidly evolving field, so I will use a web search to find the most up-to-date information.\n",
            "Action: [Web Search]\n",
            "Action Input: \"recent innovations drought resistant crops\"\u001b[0m[Web Search] is not a valid tool, try one of [Web Search].\u001b[32;1m\u001b[1;3mI apologize, I seem to be having trouble accessing the web search tool. I will try to provide a general answer based on my existing knowledge, but please be aware that this information may not be the absolute latest.\n",
            "\n",
            "Generally, recent advancements in drought-resistant crops focus on several key areas:\n",
            "\n",
            "*   **Genetic Modification (GM):** Scientists are using genetic engineering to introduce genes that enhance drought tolerance. This includes genes that regulate water use efficiency, improve root development, and increase stress tolerance. Examples include drought-tolerant corn varieties.\n",
            "\n",
            "*   **Gene Editing (e.g., CRISPR):** Gene editing techniques offer a more precise way to modify a plant's DNA to improve drought resistance. This approach can be faster and potentially less controversial than traditional GM methods.\n",
            "\n",
            "*   **Marker-Assisted Selection (MAS):** MAS uses DNA markers to identify plants with desirable drought-resistant traits, allowing breeders to select and crossbreed these plants more efficiently.\n",
            "\n",
            "*   **Traditional Breeding:** Conventional breeding programs continue to play a vital role in developing drought-resistant varieties. Breeders select plants that perform well under drought conditions and cross them to create improved varieties.\n",
            "\n",
            "*   **Improved Irrigation Techniques:** While not directly related to crop genetics, advancements in irrigation technologies like drip irrigation and precision irrigation help to conserve water and improve crop yields in drought-prone areas.\n",
            "\n",
            "*   **Soil Management Practices:** Techniques like conservation tillage, cover cropping, and the use of soil amendments can improve soil health and water retention, making crops more resilient to drought.\n",
            "\n",
            "For the most up-to-date information, I recommend searching for recent scientific publications and agricultural extension articles on drought-resistant crops.\n",
            "Final Answer: Recent advancements in drought-resistant crops focus on genetic modification, gene editing, marker-assisted selection, traditional breeding, improved irrigation techniques, and soil management practices. For the most up-to-date information, I recommend searching for recent scientific publications and agricultural extension articles on drought-resistant crops.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Agent Output: Recent advancements in drought-resistant crops focus on genetic modification, gene editing, marker-assisted selection, traditional breeding, improved irrigation techniques, and soil management practices. For the most up-to-date information, I recommend searching for recent scientific publications and agricultural extension articles on drought-resistant crops.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ifPsBQ7R27I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RjYsEIF4BV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}